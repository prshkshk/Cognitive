{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, Embedding, Flatten, Input, LSTM, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Configuration\n",
    "#\n",
    "MAX_NB_WORDS=25000\n",
    "MAX_SEQUENCE_LENGTH=1000\n",
    "N_GLOVE_TOKENS=400000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading negative reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12500/12500 [01:12<00:00, 173.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading positive reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12500/12500 [01:11<00:00, 175.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = (25000, 1000), labels.shape = (25000, 2)\n",
      "Loading word embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 400000/400000 [00:14<00:00, 28484.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape = (88583, 100)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load the data\n",
    "#\n",
    "positive_dir = \"aclImdb/train/pos\"\n",
    "negative_dir = \"aclImdb/train/neg\"\n",
    "\n",
    "def read_text(filename):\n",
    "        with open(filename,encoding='utf8') as f:\n",
    "                return f.read().lower()\n",
    "\n",
    "print (\"Reading negative reviews.\")\n",
    "negative_text = [read_text(os.path.join(negative_dir, filename))\n",
    "        for filename in tqdm.tqdm(os.listdir(negative_dir))]\n",
    "        \n",
    "print (\"Reading positive reviews.\")\n",
    "positive_text = [read_text(os.path.join(positive_dir, filename))\n",
    "        for filename in tqdm.tqdm(os.listdir(positive_dir))]\n",
    "\n",
    "\n",
    "labels_index = { \"negative\": 0, \"positive\": 1 }\n",
    "\n",
    "labels = [0 for _ in range(len(negative_text))] + \\\n",
    "        [1 for _ in range(len(negative_text))]\n",
    "    \n",
    "texts = negative_text + positive_text\n",
    " \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np_utils.to_categorical(np.asarray(labels))\n",
    "print (\"data.shape = {0}, labels.shape = {1}\".format(data.shape, labels.shape))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "\n",
    "#\n",
    "# Load word embeddings\n",
    "#\n",
    "print(\"Loading word embeddings.\")\n",
    "embeddings_index = dict()\n",
    "with open(\"glove.6B.100d.txt\",encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, total=N_GLOVE_TOKENS):\n",
    "                values = line.split()\n",
    "                word, coefficients = values[0], np.asarray(values[1:], dtype=np.float32)\n",
    "                embeddings_index[word] = coefficients\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print (\"embedding_matrix.shape = {0}\".format(embedding_matrix.shape))\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prshk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/4\n",
      "18750/18750 [==============================] - ETA: 4:06 - loss: 0.7055 - acc: 0.437 - ETA: 3:53 - loss: 0.8292 - acc: 0.519 - ETA: 3:50 - loss: 0.7862 - acc: 0.523 - ETA: 3:46 - loss: 0.7642 - acc: 0.531 - ETA: 3:44 - loss: 0.7579 - acc: 0.515 - ETA: 3:42 - loss: 0.7481 - acc: 0.523 - ETA: 3:41 - loss: 0.7416 - acc: 0.519 - ETA: 3:39 - loss: 0.7384 - acc: 0.515 - ETA: 3:37 - loss: 0.7344 - acc: 0.510 - ETA: 3:35 - loss: 0.7310 - acc: 0.507 - ETA: 3:33 - loss: 0.7275 - acc: 0.508 - ETA: 3:32 - loss: 0.7245 - acc: 0.511 - ETA: 3:30 - loss: 0.7227 - acc: 0.508 - ETA: 3:28 - loss: 0.7203 - acc: 0.508 - ETA: 3:26 - loss: 0.7190 - acc: 0.506 - ETA: 3:27 - loss: 0.7177 - acc: 0.503 - ETA: 3:27 - loss: 0.7161 - acc: 0.502 - ETA: 3:27 - loss: 0.7148 - acc: 0.500 - ETA: 3:26 - loss: 0.7138 - acc: 0.501 - ETA: 3:24 - loss: 0.7139 - acc: 0.498 - ETA: 3:23 - loss: 0.7131 - acc: 0.497 - ETA: 3:22 - loss: 0.7123 - acc: 0.498 - ETA: 3:20 - loss: 0.7118 - acc: 0.497 - ETA: 3:19 - loss: 0.7111 - acc: 0.495 - ETA: 3:19 - loss: 0.7103 - acc: 0.496 - ETA: 3:18 - loss: 0.7097 - acc: 0.495 - ETA: 3:16 - loss: 0.7090 - acc: 0.497 - ETA: 3:15 - loss: 0.7085 - acc: 0.494 - ETA: 3:13 - loss: 0.7080 - acc: 0.493 - ETA: 3:11 - loss: 0.7073 - acc: 0.495 - ETA: 3:09 - loss: 0.7069 - acc: 0.495 - ETA: 3:08 - loss: 0.7066 - acc: 0.494 - ETA: 3:06 - loss: 0.7065 - acc: 0.493 - ETA: 3:04 - loss: 0.7059 - acc: 0.496 - ETA: 3:02 - loss: 0.7056 - acc: 0.496 - ETA: 3:01 - loss: 0.7053 - acc: 0.497 - ETA: 2:59 - loss: 0.7050 - acc: 0.497 - ETA: 2:57 - loss: 0.7046 - acc: 0.497 - ETA: 2:55 - loss: 0.7043 - acc: 0.499 - ETA: 2:54 - loss: 0.7041 - acc: 0.498 - ETA: 2:52 - loss: 0.7038 - acc: 0.498 - ETA: 2:50 - loss: 0.7036 - acc: 0.497 - ETA: 2:49 - loss: 0.7033 - acc: 0.499 - ETA: 2:47 - loss: 0.7030 - acc: 0.499 - ETA: 2:45 - loss: 0.7028 - acc: 0.500 - ETA: 2:43 - loss: 0.7026 - acc: 0.499 - ETA: 2:42 - loss: 0.7024 - acc: 0.500 - ETA: 2:40 - loss: 0.7022 - acc: 0.499 - ETA: 2:38 - loss: 0.7020 - acc: 0.499 - ETA: 2:37 - loss: 0.7016 - acc: 0.500 - ETA: 2:35 - loss: 0.7014 - acc: 0.502 - ETA: 2:33 - loss: 0.7011 - acc: 0.504 - ETA: 2:32 - loss: 0.7009 - acc: 0.505 - ETA: 2:30 - loss: 0.7008 - acc: 0.506 - ETA: 2:28 - loss: 0.7007 - acc: 0.506 - ETA: 2:27 - loss: 0.7001 - acc: 0.508 - ETA: 2:25 - loss: 0.7000 - acc: 0.509 - ETA: 2:24 - loss: 0.6998 - acc: 0.510 - ETA: 2:22 - loss: 0.6999 - acc: 0.509 - ETA: 2:21 - loss: 0.6996 - acc: 0.509 - ETA: 2:19 - loss: 0.6994 - acc: 0.510 - ETA: 2:17 - loss: 0.6992 - acc: 0.511 - ETA: 2:16 - loss: 0.6996 - acc: 0.510 - ETA: 2:14 - loss: 0.6999 - acc: 0.509 - ETA: 2:12 - loss: 0.7001 - acc: 0.508 - ETA: 2:11 - loss: 0.6997 - acc: 0.510 - ETA: 2:09 - loss: 0.6995 - acc: 0.511 - ETA: 2:08 - loss: 0.6994 - acc: 0.512 - ETA: 2:06 - loss: 0.6992 - acc: 0.513 - ETA: 2:04 - loss: 0.6991 - acc: 0.514 - ETA: 2:03 - loss: 0.6989 - acc: 0.516 - ETA: 2:01 - loss: 0.6988 - acc: 0.516 - ETA: 1:59 - loss: 0.6986 - acc: 0.517 - ETA: 1:58 - loss: 0.6984 - acc: 0.518 - ETA: 1:56 - loss: 0.6983 - acc: 0.520 - ETA: 1:54 - loss: 0.6981 - acc: 0.521 - ETA: 1:53 - loss: 0.6979 - acc: 0.523 - ETA: 1:51 - loss: 0.6976 - acc: 0.525 - ETA: 1:49 - loss: 0.6971 - acc: 0.527 - ETA: 1:48 - loss: 0.6969 - acc: 0.528 - ETA: 1:46 - loss: 0.6965 - acc: 0.529 - ETA: 1:45 - loss: 0.6960 - acc: 0.530 - ETA: 1:43 - loss: 0.6954 - acc: 0.531 - ETA: 1:41 - loss: 0.6950 - acc: 0.531 - ETA: 1:40 - loss: 0.6943 - acc: 0.532 - ETA: 1:38 - loss: 0.6939 - acc: 0.534 - ETA: 1:36 - loss: 0.6930 - acc: 0.535 - ETA: 1:35 - loss: 0.6928 - acc: 0.536 - ETA: 1:33 - loss: 0.6920 - acc: 0.538 - ETA: 1:31 - loss: 0.6902 - acc: 0.540 - ETA: 1:30 - loss: 0.6890 - acc: 0.542 - ETA: 1:28 - loss: 0.6874 - acc: 0.544 - ETA: 1:26 - loss: 0.6867 - acc: 0.546 - ETA: 1:25 - loss: 0.6851 - acc: 0.548 - ETA: 1:23 - loss: 0.6843 - acc: 0.549 - ETA: 1:22 - loss: 0.6835 - acc: 0.551 - ETA: 1:20 - loss: 0.6830 - acc: 0.552 - ETA: 1:18 - loss: 0.6816 - acc: 0.554 - ETA: 1:17 - loss: 0.6816 - acc: 0.555 - ETA: 1:15 - loss: 0.6799 - acc: 0.557 - ETA: 1:14 - loss: 0.6807 - acc: 0.557 - ETA: 1:12 - loss: 0.6793 - acc: 0.559 - ETA: 1:11 - loss: 0.6792 - acc: 0.560 - ETA: 1:09 - loss: 0.6798 - acc: 0.560 - ETA: 1:08 - loss: 0.6795 - acc: 0.560 - ETA: 1:06 - loss: 0.6782 - acc: 0.562 - ETA: 1:04 - loss: 0.6786 - acc: 0.562 - ETA: 1:03 - loss: 0.6787 - acc: 0.561 - ETA: 1:01 - loss: 0.6782 - acc: 0.562 - ETA: 59s - loss: 0.6776 - acc: 0.564 - ETA: 58s - loss: 0.6771 - acc: 0.56 - ETA: 56s - loss: 0.6766 - acc: 0.56 - ETA: 55s - loss: 0.6763 - acc: 0.56 - ETA: 53s - loss: 0.6762 - acc: 0.56 - ETA: 51s - loss: 0.6763 - acc: 0.56 - ETA: 50s - loss: 0.6757 - acc: 0.56 - ETA: 48s - loss: 0.6752 - acc: 0.56 - ETA: 46s - loss: 0.6745 - acc: 0.56 - ETA: 45s - loss: 0.6738 - acc: 0.57 - ETA: 43s - loss: 0.6731 - acc: 0.57 - ETA: 41s - loss: 0.6727 - acc: 0.57 - ETA: 40s - loss: 0.6718 - acc: 0.57 - ETA: 38s - loss: 0.6702 - acc: 0.57 - ETA: 37s - loss: 0.6693 - acc: 0.57 - ETA: 35s - loss: 0.6679 - acc: 0.57 - ETA: 33s - loss: 0.6667 - acc: 0.57 - ETA: 32s - loss: 0.6648 - acc: 0.58 - ETA: 30s - loss: 0.6634 - acc: 0.58 - ETA: 28s - loss: 0.6619 - acc: 0.58 - ETA: 27s - loss: 0.6615 - acc: 0.58 - ETA: 25s - loss: 0.6608 - acc: 0.58 - ETA: 23s - loss: 0.6588 - acc: 0.58 - ETA: 22s - loss: 0.6583 - acc: 0.58 - ETA: 20s - loss: 0.6567 - acc: 0.59 - ETA: 19s - loss: 0.6560 - acc: 0.59 - ETA: 17s - loss: 0.6544 - acc: 0.59 - ETA: 15s - loss: 0.6526 - acc: 0.59 - ETA: 14s - loss: 0.6512 - acc: 0.59 - ETA: 12s - loss: 0.6498 - acc: 0.59 - ETA: 10s - loss: 0.6485 - acc: 0.59 - ETA: 9s - loss: 0.6469 - acc: 0.6006 - ETA: 7s - loss: 0.6455 - acc: 0.601 - ETA: 5s - loss: 0.6444 - acc: 0.603 - ETA: 4s - loss: 0.6437 - acc: 0.604 - ETA: 2s - loss: 0.6424 - acc: 0.605 - ETA: 0s - loss: 0.6406 - acc: 0.606 - 285s 15ms/step - loss: 0.6398 - acc: 0.6074 - val_loss: 0.4489 - val_acc: 0.7968\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - ETA: 4:42 - loss: 0.4824 - acc: 0.789 - ETA: 4:52 - loss: 0.4835 - acc: 0.773 - ETA: 5:06 - loss: 0.4699 - acc: 0.789 - ETA: 4:58 - loss: 0.4287 - acc: 0.814 - ETA: 4:49 - loss: 0.4266 - acc: 0.807 - ETA: 4:44 - loss: 0.4157 - acc: 0.809 - ETA: 4:45 - loss: 0.4247 - acc: 0.805 - ETA: 4:50 - loss: 0.4415 - acc: 0.794 - ETA: 4:47 - loss: 0.4366 - acc: 0.800 - ETA: 4:45 - loss: 0.4418 - acc: 0.798 - ETA: 4:52 - loss: 0.4343 - acc: 0.802 - ETA: 4:54 - loss: 0.4341 - acc: 0.800 - ETA: 4:51 - loss: 0.4336 - acc: 0.800 - ETA: 4:57 - loss: 0.4327 - acc: 0.800 - ETA: 4:58 - loss: 0.4349 - acc: 0.797 - ETA: 4:54 - loss: 0.4353 - acc: 0.797 - ETA: 4:49 - loss: 0.4356 - acc: 0.797 - ETA: 4:45 - loss: 0.4289 - acc: 0.803 - ETA: 4:40 - loss: 0.4301 - acc: 0.803 - ETA: 4:36 - loss: 0.4276 - acc: 0.807 - ETA: 4:32 - loss: 0.4285 - acc: 0.806 - ETA: 4:28 - loss: 0.4279 - acc: 0.805 - ETA: 4:26 - loss: 0.4261 - acc: 0.807 - ETA: 4:22 - loss: 0.4237 - acc: 0.808 - ETA: 4:19 - loss: 0.4268 - acc: 0.806 - ETA: 4:16 - loss: 0.4270 - acc: 0.805 - ETA: 4:13 - loss: 0.4278 - acc: 0.804 - ETA: 4:09 - loss: 0.4260 - acc: 0.805 - ETA: 4:06 - loss: 0.4286 - acc: 0.805 - ETA: 4:03 - loss: 0.4259 - acc: 0.807 - ETA: 4:00 - loss: 0.4245 - acc: 0.807 - ETA: 3:58 - loss: 0.4258 - acc: 0.807 - ETA: 3:55 - loss: 0.4242 - acc: 0.808 - ETA: 3:53 - loss: 0.4246 - acc: 0.807 - ETA: 3:50 - loss: 0.4266 - acc: 0.808 - ETA: 3:48 - loss: 0.4283 - acc: 0.806 - ETA: 3:45 - loss: 0.4286 - acc: 0.807 - ETA: 3:43 - loss: 0.4269 - acc: 0.807 - ETA: 3:40 - loss: 0.4267 - acc: 0.807 - ETA: 3:37 - loss: 0.4271 - acc: 0.807 - ETA: 3:35 - loss: 0.4249 - acc: 0.809 - ETA: 3:33 - loss: 0.4238 - acc: 0.810 - ETA: 3:31 - loss: 0.4221 - acc: 0.811 - ETA: 3:28 - loss: 0.4199 - acc: 0.813 - ETA: 3:26 - loss: 0.4201 - acc: 0.813 - ETA: 3:24 - loss: 0.4184 - acc: 0.813 - ETA: 3:21 - loss: 0.4182 - acc: 0.814 - ETA: 3:19 - loss: 0.4162 - acc: 0.814 - ETA: 3:16 - loss: 0.4150 - acc: 0.816 - ETA: 3:14 - loss: 0.4130 - acc: 0.816 - ETA: 3:12 - loss: 0.4117 - acc: 0.816 - ETA: 3:09 - loss: 0.4119 - acc: 0.816 - ETA: 3:07 - loss: 0.4105 - acc: 0.816 - ETA: 3:05 - loss: 0.4165 - acc: 0.813 - ETA: 3:03 - loss: 0.4182 - acc: 0.811 - ETA: 3:00 - loss: 0.4232 - acc: 0.809 - ETA: 2:58 - loss: 0.4250 - acc: 0.808 - ETA: 2:56 - loss: 0.4247 - acc: 0.808 - ETA: 2:54 - loss: 0.4246 - acc: 0.808 - ETA: 2:52 - loss: 0.4274 - acc: 0.806 - ETA: 2:50 - loss: 0.4286 - acc: 0.805 - ETA: 2:47 - loss: 0.4296 - acc: 0.805 - ETA: 2:45 - loss: 0.4308 - acc: 0.804 - ETA: 2:43 - loss: 0.4317 - acc: 0.804 - ETA: 2:41 - loss: 0.4330 - acc: 0.803 - ETA: 2:39 - loss: 0.4331 - acc: 0.804 - ETA: 2:37 - loss: 0.4329 - acc: 0.805 - ETA: 2:35 - loss: 0.4334 - acc: 0.804 - ETA: 2:33 - loss: 0.4344 - acc: 0.804 - ETA: 2:31 - loss: 0.4347 - acc: 0.803 - ETA: 2:29 - loss: 0.4349 - acc: 0.803 - ETA: 2:27 - loss: 0.4347 - acc: 0.804 - ETA: 2:25 - loss: 0.4346 - acc: 0.804 - ETA: 2:22 - loss: 0.4341 - acc: 0.804 - ETA: 2:20 - loss: 0.4333 - acc: 0.805 - ETA: 2:18 - loss: 0.4327 - acc: 0.805 - ETA: 2:16 - loss: 0.4327 - acc: 0.806 - ETA: 2:14 - loss: 0.4339 - acc: 0.805 - ETA: 2:12 - loss: 0.4337 - acc: 0.805 - ETA: 2:10 - loss: 0.4331 - acc: 0.806 - ETA: 2:08 - loss: 0.4326 - acc: 0.806 - ETA: 2:06 - loss: 0.4339 - acc: 0.805 - ETA: 2:04 - loss: 0.4328 - acc: 0.806 - ETA: 2:02 - loss: 0.4323 - acc: 0.806 - ETA: 2:00 - loss: 0.4331 - acc: 0.805 - ETA: 1:58 - loss: 0.4341 - acc: 0.805 - ETA: 1:56 - loss: 0.4342 - acc: 0.805 - ETA: 1:54 - loss: 0.4350 - acc: 0.804 - ETA: 1:52 - loss: 0.4364 - acc: 0.804 - ETA: 1:50 - loss: 0.4355 - acc: 0.804 - ETA: 1:48 - loss: 0.4348 - acc: 0.804 - ETA: 1:46 - loss: 0.4360 - acc: 0.803 - ETA: 1:44 - loss: 0.4354 - acc: 0.803 - ETA: 1:42 - loss: 0.4351 - acc: 0.804 - ETA: 1:40 - loss: 0.4341 - acc: 0.805 - ETA: 1:38 - loss: 0.4339 - acc: 0.805 - ETA: 1:36 - loss: 0.4340 - acc: 0.805 - ETA: 1:34 - loss: 0.4331 - acc: 0.805 - ETA: 1:32 - loss: 0.4325 - acc: 0.805 - ETA: 1:31 - loss: 0.4329 - acc: 0.805 - ETA: 1:29 - loss: 0.4331 - acc: 0.805 - ETA: 1:27 - loss: 0.4332 - acc: 0.805 - ETA: 1:25 - loss: 0.4329 - acc: 0.805 - ETA: 1:23 - loss: 0.4326 - acc: 0.806 - ETA: 1:21 - loss: 0.4323 - acc: 0.806 - ETA: 1:19 - loss: 0.4319 - acc: 0.806 - ETA: 1:17 - loss: 0.4314 - acc: 0.805 - ETA: 1:15 - loss: 0.4313 - acc: 0.805 - ETA: 1:13 - loss: 0.4328 - acc: 0.805 - ETA: 1:11 - loss: 0.4325 - acc: 0.805 - ETA: 1:09 - loss: 0.4318 - acc: 0.805 - ETA: 1:07 - loss: 0.4310 - acc: 0.806 - ETA: 1:05 - loss: 0.4313 - acc: 0.806 - ETA: 1:03 - loss: 0.4312 - acc: 0.806 - ETA: 1:01 - loss: 0.4306 - acc: 0.806 - ETA: 59s - loss: 0.4300 - acc: 0.806 - ETA: 57s - loss: 0.4293 - acc: 0.80 - ETA: 55s - loss: 0.4299 - acc: 0.80 - ETA: 53s - loss: 0.4294 - acc: 0.80 - ETA: 51s - loss: 0.4287 - acc: 0.80 - ETA: 49s - loss: 0.4282 - acc: 0.80 - ETA: 47s - loss: 0.4276 - acc: 0.80 - ETA: 45s - loss: 0.4271 - acc: 0.80 - ETA: 43s - loss: 0.4266 - acc: 0.80 - ETA: 41s - loss: 0.4265 - acc: 0.80 - ETA: 39s - loss: 0.4266 - acc: 0.80 - ETA: 38s - loss: 0.4264 - acc: 0.80 - ETA: 36s - loss: 0.4258 - acc: 0.80 - ETA: 34s - loss: 0.4251 - acc: 0.80 - ETA: 32s - loss: 0.4244 - acc: 0.80 - ETA: 30s - loss: 0.4239 - acc: 0.81 - ETA: 28s - loss: 0.4232 - acc: 0.81 - ETA: 26s - loss: 0.4221 - acc: 0.81 - ETA: 24s - loss: 0.4215 - acc: 0.81 - ETA: 22s - loss: 0.4206 - acc: 0.81 - ETA: 20s - loss: 0.4199 - acc: 0.81 - ETA: 18s - loss: 0.4197 - acc: 0.81 - ETA: 16s - loss: 0.4189 - acc: 0.81 - ETA: 14s - loss: 0.4189 - acc: 0.81 - ETA: 12s - loss: 0.4184 - acc: 0.81 - ETA: 10s - loss: 0.4181 - acc: 0.81 - ETA: 8s - loss: 0.4179 - acc: 0.8130 - ETA: 6s - loss: 0.4178 - acc: 0.813 - ETA: 4s - loss: 0.4180 - acc: 0.813 - ETA: 2s - loss: 0.4174 - acc: 0.813 - ETA: 0s - loss: 0.4168 - acc: 0.813 - 336s 18ms/step - loss: 0.4167 - acc: 0.8139 - val_loss: 0.3506 - val_acc: 0.8478\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - ETA: 4:59 - loss: 0.3793 - acc: 0.820 - ETA: 4:52 - loss: 0.3471 - acc: 0.847 - ETA: 4:46 - loss: 0.3361 - acc: 0.854 - ETA: 4:45 - loss: 0.3107 - acc: 0.873 - ETA: 4:42 - loss: 0.3118 - acc: 0.870 - ETA: 4:43 - loss: 0.3175 - acc: 0.862 - ETA: 4:43 - loss: 0.3142 - acc: 0.863 - ETA: 4:41 - loss: 0.3116 - acc: 0.861 - ETA: 4:40 - loss: 0.3076 - acc: 0.866 - ETA: 4:54 - loss: 0.3053 - acc: 0.868 - ETA: 5:06 - loss: 0.3088 - acc: 0.869 - ETA: 5:05 - loss: 0.3100 - acc: 0.867 - ETA: 5:06 - loss: 0.3104 - acc: 0.868 - ETA: 5:04 - loss: 0.3068 - acc: 0.870 - ETA: 4:58 - loss: 0.3082 - acc: 0.872 - ETA: 4:53 - loss: 0.3059 - acc: 0.872 - ETA: 4:48 - loss: 0.3087 - acc: 0.870 - ETA: 4:43 - loss: 0.3046 - acc: 0.872 - ETA: 4:39 - loss: 0.3071 - acc: 0.871 - ETA: 4:35 - loss: 0.3089 - acc: 0.871 - ETA: 4:31 - loss: 0.3112 - acc: 0.870 - ETA: 4:27 - loss: 0.3145 - acc: 0.870 - ETA: 4:24 - loss: 0.3202 - acc: 0.866 - ETA: 4:20 - loss: 0.3188 - acc: 0.867 - ETA: 4:17 - loss: 0.3188 - acc: 0.866 - ETA: 4:14 - loss: 0.3219 - acc: 0.866 - ETA: 4:10 - loss: 0.3229 - acc: 0.864 - ETA: 4:07 - loss: 0.3243 - acc: 0.864 - ETA: 4:04 - loss: 0.3252 - acc: 0.864 - ETA: 4:02 - loss: 0.3264 - acc: 0.863 - ETA: 4:00 - loss: 0.3254 - acc: 0.863 - ETA: 3:57 - loss: 0.3253 - acc: 0.864 - ETA: 3:55 - loss: 0.3246 - acc: 0.865 - ETA: 3:52 - loss: 0.3235 - acc: 0.865 - ETA: 3:49 - loss: 0.3242 - acc: 0.865 - ETA: 3:46 - loss: 0.3253 - acc: 0.865 - ETA: 3:44 - loss: 0.3246 - acc: 0.865 - ETA: 3:42 - loss: 0.3234 - acc: 0.865 - ETA: 3:40 - loss: 0.3217 - acc: 0.866 - ETA: 3:37 - loss: 0.3229 - acc: 0.866 - ETA: 3:35 - loss: 0.3261 - acc: 0.863 - ETA: 3:32 - loss: 0.3265 - acc: 0.863 - ETA: 3:30 - loss: 0.3254 - acc: 0.864 - ETA: 3:27 - loss: 0.3252 - acc: 0.864 - ETA: 3:25 - loss: 0.3237 - acc: 0.865 - ETA: 3:23 - loss: 0.3234 - acc: 0.864 - ETA: 3:20 - loss: 0.3248 - acc: 0.864 - ETA: 3:18 - loss: 0.3247 - acc: 0.864 - ETA: 3:16 - loss: 0.3232 - acc: 0.865 - ETA: 3:14 - loss: 0.3237 - acc: 0.865 - ETA: 3:12 - loss: 0.3227 - acc: 0.865 - ETA: 3:09 - loss: 0.3217 - acc: 0.865 - ETA: 3:07 - loss: 0.3211 - acc: 0.865 - ETA: 3:05 - loss: 0.3204 - acc: 0.866 - ETA: 3:03 - loss: 0.3195 - acc: 0.866 - ETA: 3:00 - loss: 0.3181 - acc: 0.867 - ETA: 2:58 - loss: 0.3175 - acc: 0.867 - ETA: 2:56 - loss: 0.3177 - acc: 0.867 - ETA: 2:54 - loss: 0.3174 - acc: 0.867 - ETA: 2:52 - loss: 0.3174 - acc: 0.867 - ETA: 2:50 - loss: 0.3165 - acc: 0.867 - ETA: 2:47 - loss: 0.3169 - acc: 0.867 - ETA: 2:45 - loss: 0.3158 - acc: 0.867 - ETA: 2:43 - loss: 0.3161 - acc: 0.867 - ETA: 2:41 - loss: 0.3161 - acc: 0.867 - ETA: 2:39 - loss: 0.3156 - acc: 0.867 - ETA: 2:37 - loss: 0.3145 - acc: 0.867 - ETA: 2:36 - loss: 0.3152 - acc: 0.867 - ETA: 2:34 - loss: 0.3143 - acc: 0.868 - ETA: 2:32 - loss: 0.3137 - acc: 0.868 - ETA: 2:32 - loss: 0.3144 - acc: 0.867 - ETA: 2:30 - loss: 0.3135 - acc: 0.868 - ETA: 2:28 - loss: 0.3134 - acc: 0.868 - ETA: 2:26 - loss: 0.3131 - acc: 0.868 - ETA: 2:24 - loss: 0.3126 - acc: 0.868 - ETA: 2:23 - loss: 0.3121 - acc: 0.868 - ETA: 2:21 - loss: 0.3110 - acc: 0.869 - ETA: 2:19 - loss: 0.3097 - acc: 0.870 - ETA: 2:17 - loss: 0.3092 - acc: 0.870 - ETA: 2:15 - loss: 0.3104 - acc: 0.870 - ETA: 2:13 - loss: 0.3110 - acc: 0.870 - ETA: 2:11 - loss: 0.3130 - acc: 0.869 - ETA: 2:09 - loss: 0.3117 - acc: 0.870 - ETA: 2:07 - loss: 0.3113 - acc: 0.870 - ETA: 2:05 - loss: 0.3121 - acc: 0.869 - ETA: 2:02 - loss: 0.3134 - acc: 0.868 - ETA: 2:00 - loss: 0.3144 - acc: 0.868 - ETA: 1:58 - loss: 0.3150 - acc: 0.867 - ETA: 1:56 - loss: 0.3151 - acc: 0.867 - ETA: 1:54 - loss: 0.3148 - acc: 0.867 - ETA: 1:52 - loss: 0.3145 - acc: 0.868 - ETA: 1:50 - loss: 0.3151 - acc: 0.867 - ETA: 1:47 - loss: 0.3160 - acc: 0.867 - ETA: 1:45 - loss: 0.3157 - acc: 0.867 - ETA: 1:43 - loss: 0.3170 - acc: 0.866 - ETA: 1:41 - loss: 0.3180 - acc: 0.865 - ETA: 1:39 - loss: 0.3178 - acc: 0.866 - ETA: 1:37 - loss: 0.3189 - acc: 0.865 - ETA: 1:35 - loss: 0.3189 - acc: 0.865 - ETA: 1:33 - loss: 0.3190 - acc: 0.865 - ETA: 1:31 - loss: 0.3186 - acc: 0.865 - ETA: 1:29 - loss: 0.3183 - acc: 0.866 - ETA: 1:28 - loss: 0.3179 - acc: 0.866 - ETA: 1:26 - loss: 0.3185 - acc: 0.865 - ETA: 1:24 - loss: 0.3195 - acc: 0.865 - ETA: 1:22 - loss: 0.3193 - acc: 0.865 - ETA: 1:20 - loss: 0.3190 - acc: 0.865 - ETA: 1:18 - loss: 0.3191 - acc: 0.865 - ETA: 1:16 - loss: 0.3197 - acc: 0.864 - ETA: 1:14 - loss: 0.3196 - acc: 0.865 - ETA: 1:12 - loss: 0.3195 - acc: 0.865 - ETA: 1:10 - loss: 0.3201 - acc: 0.865 - ETA: 1:07 - loss: 0.3201 - acc: 0.865 - ETA: 1:05 - loss: 0.3208 - acc: 0.865 - ETA: 1:03 - loss: 0.3210 - acc: 0.864 - ETA: 1:01 - loss: 0.3210 - acc: 0.864 - ETA: 59s - loss: 0.3205 - acc: 0.865 - ETA: 57s - loss: 0.3209 - acc: 0.86 - ETA: 55s - loss: 0.3207 - acc: 0.86 - ETA: 53s - loss: 0.3205 - acc: 0.86 - ETA: 51s - loss: 0.3205 - acc: 0.86 - ETA: 49s - loss: 0.3203 - acc: 0.86 - ETA: 47s - loss: 0.3195 - acc: 0.86 - ETA: 45s - loss: 0.3191 - acc: 0.86 - ETA: 43s - loss: 0.3187 - acc: 0.86 - ETA: 41s - loss: 0.3184 - acc: 0.86 - ETA: 39s - loss: 0.3182 - acc: 0.86 - ETA: 37s - loss: 0.3183 - acc: 0.86 - ETA: 35s - loss: 0.3189 - acc: 0.86 - ETA: 33s - loss: 0.3188 - acc: 0.86 - ETA: 31s - loss: 0.3186 - acc: 0.86 - ETA: 29s - loss: 0.3181 - acc: 0.86 - ETA: 27s - loss: 0.3183 - acc: 0.86 - ETA: 25s - loss: 0.3180 - acc: 0.86 - ETA: 23s - loss: 0.3178 - acc: 0.86 - ETA: 21s - loss: 0.3183 - acc: 0.86 - ETA: 19s - loss: 0.3185 - acc: 0.86 - ETA: 17s - loss: 0.3187 - acc: 0.86 - ETA: 15s - loss: 0.3186 - acc: 0.86 - ETA: 13s - loss: 0.3184 - acc: 0.86 - ETA: 11s - loss: 0.3189 - acc: 0.86 - ETA: 9s - loss: 0.3190 - acc: 0.8664 - ETA: 7s - loss: 0.3187 - acc: 0.866 - ETA: 5s - loss: 0.3186 - acc: 0.866 - ETA: 3s - loss: 0.3187 - acc: 0.866 - ETA: 0s - loss: 0.3187 - acc: 0.866 - 345s 18ms/step - loss: 0.3185 - acc: 0.8669 - val_loss: 0.3289 - val_acc: 0.8566\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - ETA: 4:54 - loss: 0.2689 - acc: 0.937 - ETA: 4:48 - loss: 0.2600 - acc: 0.910 - ETA: 4:45 - loss: 0.2521 - acc: 0.914 - ETA: 4:43 - loss: 0.2438 - acc: 0.914 - ETA: 4:42 - loss: 0.2327 - acc: 0.915 - ETA: 4:40 - loss: 0.2343 - acc: 0.915 - ETA: 4:38 - loss: 0.2351 - acc: 0.912 - ETA: 4:38 - loss: 0.2305 - acc: 0.913 - ETA: 4:39 - loss: 0.2290 - acc: 0.913 - ETA: 4:35 - loss: 0.2318 - acc: 0.911 - ETA: 4:32 - loss: 0.2317 - acc: 0.909 - ETA: 4:29 - loss: 0.2337 - acc: 0.907 - ETA: 4:27 - loss: 0.2376 - acc: 0.904 - ETA: 4:24 - loss: 0.2456 - acc: 0.901 - ETA: 4:21 - loss: 0.2487 - acc: 0.899 - ETA: 4:18 - loss: 0.2519 - acc: 0.898 - ETA: 4:15 - loss: 0.2476 - acc: 0.902 - ETA: 4:12 - loss: 0.2472 - acc: 0.901 - ETA: 4:10 - loss: 0.2532 - acc: 0.898 - ETA: 4:08 - loss: 0.2570 - acc: 0.896 - ETA: 4:05 - loss: 0.2584 - acc: 0.896 - ETA: 4:03 - loss: 0.2602 - acc: 0.894 - ETA: 4:00 - loss: 0.2604 - acc: 0.895 - ETA: 3:59 - loss: 0.2620 - acc: 0.895 - ETA: 3:57 - loss: 0.2622 - acc: 0.894 - ETA: 3:55 - loss: 0.2610 - acc: 0.896 - ETA: 3:52 - loss: 0.2593 - acc: 0.898 - ETA: 3:50 - loss: 0.2582 - acc: 0.900 - ETA: 3:49 - loss: 0.2577 - acc: 0.901 - ETA: 3:47 - loss: 0.2567 - acc: 0.901 - ETA: 3:45 - loss: 0.2559 - acc: 0.901 - ETA: 3:44 - loss: 0.2545 - acc: 0.901 - ETA: 3:43 - loss: 0.2554 - acc: 0.901 - ETA: 3:41 - loss: 0.2555 - acc: 0.900 - ETA: 3:40 - loss: 0.2552 - acc: 0.900 - ETA: 3:38 - loss: 0.2550 - acc: 0.900 - ETA: 3:36 - loss: 0.2554 - acc: 0.900 - ETA: 3:34 - loss: 0.2561 - acc: 0.900 - ETA: 3:32 - loss: 0.2573 - acc: 0.899 - ETA: 3:31 - loss: 0.2569 - acc: 0.899 - ETA: 3:29 - loss: 0.2556 - acc: 0.900 - ETA: 3:27 - loss: 0.2544 - acc: 0.901 - ETA: 3:26 - loss: 0.2535 - acc: 0.901 - ETA: 3:23 - loss: 0.2540 - acc: 0.900 - ETA: 3:21 - loss: 0.2566 - acc: 0.900 - ETA: 3:19 - loss: 0.2569 - acc: 0.900 - ETA: 3:17 - loss: 0.2566 - acc: 0.900 - ETA: 3:16 - loss: 0.2578 - acc: 0.900 - ETA: 3:14 - loss: 0.2589 - acc: 0.899 - ETA: 3:11 - loss: 0.2573 - acc: 0.899 - ETA: 3:10 - loss: 0.2572 - acc: 0.900 - ETA: 3:09 - loss: 0.2608 - acc: 0.897 - ETA: 3:08 - loss: 0.2616 - acc: 0.897 - ETA: 3:06 - loss: 0.2618 - acc: 0.897 - ETA: 3:04 - loss: 0.2670 - acc: 0.895 - ETA: 3:02 - loss: 0.2670 - acc: 0.895 - ETA: 3:00 - loss: 0.2659 - acc: 0.895 - ETA: 2:58 - loss: 0.2679 - acc: 0.894 - ETA: 2:55 - loss: 0.2704 - acc: 0.892 - ETA: 2:53 - loss: 0.2714 - acc: 0.891 - ETA: 2:51 - loss: 0.2707 - acc: 0.892 - ETA: 2:49 - loss: 0.2714 - acc: 0.891 - ETA: 2:47 - loss: 0.2718 - acc: 0.891 - ETA: 2:45 - loss: 0.2724 - acc: 0.890 - ETA: 2:43 - loss: 0.2731 - acc: 0.890 - ETA: 2:41 - loss: 0.2723 - acc: 0.891 - ETA: 2:39 - loss: 0.2730 - acc: 0.890 - ETA: 2:37 - loss: 0.2723 - acc: 0.891 - ETA: 2:35 - loss: 0.2720 - acc: 0.891 - ETA: 2:33 - loss: 0.2728 - acc: 0.891 - ETA: 2:31 - loss: 0.2725 - acc: 0.891 - ETA: 2:29 - loss: 0.2722 - acc: 0.891 - ETA: 2:27 - loss: 0.2736 - acc: 0.890 - ETA: 2:25 - loss: 0.2741 - acc: 0.890 - ETA: 2:23 - loss: 0.2743 - acc: 0.890 - ETA: 2:22 - loss: 0.2759 - acc: 0.889 - ETA: 2:20 - loss: 0.2785 - acc: 0.888 - ETA: 2:18 - loss: 0.2782 - acc: 0.888 - ETA: 2:16 - loss: 0.2795 - acc: 0.887 - ETA: 2:14 - loss: 0.2805 - acc: 0.886 - ETA: 2:12 - loss: 0.2808 - acc: 0.887 - ETA: 2:10 - loss: 0.2805 - acc: 0.886 - ETA: 2:08 - loss: 0.2806 - acc: 0.886 - ETA: 2:06 - loss: 0.2817 - acc: 0.886 - ETA: 2:04 - loss: 0.2820 - acc: 0.886 - ETA: 2:02 - loss: 0.2822 - acc: 0.886 - ETA: 2:00 - loss: 0.2829 - acc: 0.885 - ETA: 1:58 - loss: 0.2836 - acc: 0.885 - ETA: 1:56 - loss: 0.2836 - acc: 0.885 - ETA: 1:54 - loss: 0.2830 - acc: 0.885 - ETA: 1:52 - loss: 0.2825 - acc: 0.885 - ETA: 1:50 - loss: 0.2825 - acc: 0.885 - ETA: 1:48 - loss: 0.2825 - acc: 0.885 - ETA: 1:46 - loss: 0.2826 - acc: 0.885 - ETA: 1:44 - loss: 0.2828 - acc: 0.885 - ETA: 1:42 - loss: 0.2825 - acc: 0.885 - ETA: 1:40 - loss: 0.2824 - acc: 0.885 - ETA: 1:38 - loss: 0.2814 - acc: 0.885 - ETA: 1:36 - loss: 0.2803 - acc: 0.886 - ETA: 1:34 - loss: 0.2806 - acc: 0.886 - ETA: 1:32 - loss: 0.2805 - acc: 0.886 - ETA: 1:30 - loss: 0.2801 - acc: 0.887 - ETA: 1:28 - loss: 0.2793 - acc: 0.887 - ETA: 1:26 - loss: 0.2783 - acc: 0.888 - ETA: 1:24 - loss: 0.2780 - acc: 0.887 - ETA: 1:22 - loss: 0.2770 - acc: 0.888 - ETA: 1:20 - loss: 0.2781 - acc: 0.887 - ETA: 1:18 - loss: 0.2774 - acc: 0.888 - ETA: 1:16 - loss: 0.2771 - acc: 0.888 - ETA: 1:14 - loss: 0.2769 - acc: 0.888 - ETA: 1:12 - loss: 0.2771 - acc: 0.888 - ETA: 1:10 - loss: 0.2765 - acc: 0.888 - ETA: 1:08 - loss: 0.2757 - acc: 0.888 - ETA: 1:06 - loss: 0.2773 - acc: 0.888 - ETA: 1:04 - loss: 0.2769 - acc: 0.888 - ETA: 1:02 - loss: 0.2768 - acc: 0.887 - ETA: 1:00 - loss: 0.2763 - acc: 0.888 - ETA: 57s - loss: 0.2768 - acc: 0.888 - ETA: 55s - loss: 0.2773 - acc: 0.88 - ETA: 53s - loss: 0.2771 - acc: 0.88 - ETA: 51s - loss: 0.2770 - acc: 0.88 - ETA: 49s - loss: 0.2770 - acc: 0.88 - ETA: 47s - loss: 0.2772 - acc: 0.88 - ETA: 45s - loss: 0.2768 - acc: 0.88 - ETA: 44s - loss: 0.2769 - acc: 0.88 - ETA: 42s - loss: 0.2776 - acc: 0.88 - ETA: 40s - loss: 0.2772 - acc: 0.88 - ETA: 38s - loss: 0.2772 - acc: 0.88 - ETA: 36s - loss: 0.2766 - acc: 0.88 - ETA: 34s - loss: 0.2769 - acc: 0.88 - ETA: 31s - loss: 0.2762 - acc: 0.88 - ETA: 29s - loss: 0.2759 - acc: 0.88 - ETA: 27s - loss: 0.2755 - acc: 0.88 - ETA: 25s - loss: 0.2753 - acc: 0.88 - ETA: 23s - loss: 0.2753 - acc: 0.88 - ETA: 21s - loss: 0.2750 - acc: 0.88 - ETA: 19s - loss: 0.2747 - acc: 0.88 - ETA: 17s - loss: 0.2753 - acc: 0.88 - ETA: 15s - loss: 0.2746 - acc: 0.88 - ETA: 13s - loss: 0.2735 - acc: 0.88 - ETA: 11s - loss: 0.2734 - acc: 0.88 - ETA: 9s - loss: 0.2733 - acc: 0.8892 - ETA: 7s - loss: 0.2733 - acc: 0.889 - ETA: 5s - loss: 0.2735 - acc: 0.889 - ETA: 3s - loss: 0.2732 - acc: 0.889 - ETA: 0s - loss: 0.2732 - acc: 0.889 - 347s 18ms/step - loss: 0.2733 - acc: 0.8892 - val_loss: 0.3314 - val_acc: 0.8614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b1feedf28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Build 1D ConvNet\n",
    "#\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "\n",
    "x = Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "\n",
    "\n",
    "#x = LSTM(64, dropout_W=0.2, dropout_U=0.2)(x)\n",
    "#x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "preds = Dense(len(labels_index), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])\n",
    "\n",
    "#\n",
    "# Train the model\n",
    "#\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "          nb_epoch=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Greetings and welcome to the Microsoft Fiscal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>positive</td>\n",
       "      <td>technology. Microsoft 365 helps every organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>positive</td>\n",
       "      <td>is coming from the line of Walter Pritchard wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>neutral</td>\n",
       "      <td>we're growing at eye-popping rates right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>positive</td>\n",
       "      <td>the key things that we think about is differen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "1    positive  Greetings and welcome to the Microsoft Fiscal ...\n",
       "10   positive  technology. Microsoft 365 helps every organiza...\n",
       "100  positive  is coming from the line of Walter Pritchard wi...\n",
       "101   neutral  we're growing at eye-popping rates right now. ...\n",
       "102  positive  the key things that we think about is differen..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data.text\n",
    "labels = data.sentiment.map(str.lower).map({'positive' : 1, 'neutral' : 0, 'negative' : 0}).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "test_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77918488,  0.22081508],\n",
       "       [ 0.93208724,  0.06791282],\n",
       "       [ 0.89135247,  0.10864758],\n",
       "       ..., \n",
       "       [ 0.83458674,  0.16541333],\n",
       "       [ 0.78091168,  0.21908833],\n",
       "       [ 0.82761705,  0.17238298]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = model.predict(test_data)\n",
    "labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_p = np.argmax(labels_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119, 186],\n",
       "       [120, 197]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=l_p, y_true=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         8858300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               573568    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 9,660,350\n",
      "Trainable params: 802,050\n",
      "Non-trainable params: 8,858,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
